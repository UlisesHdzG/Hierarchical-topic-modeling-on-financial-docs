{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0bc4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad115563",
   "metadata": {},
   "source": [
    "# 1. Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# read all files\n",
    "directory=r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\maildir'\n",
    "\n",
    "subset=os.listdir(directory)\n",
    "\n",
    "msg_id= list()\n",
    "msg_txt= list()\n",
    "\n",
    "for folder in subset:\n",
    "    for roots, dirs, files in os.walk(directory+\"\\\\\"+folder):\n",
    "        for file in files:\n",
    "            with open(roots+'\\\\'+file) as f:\n",
    "                x= f.readlines()\n",
    "                x=''.join(x)\n",
    "                msg_txt.append(x)\n",
    "                msg_id.append(roots[roots.index(folder):] + '\\\\'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7dda96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(zip(msg_id,msg_txt),columns=['message_id','message_text'])\n",
    "data.to_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\emails.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a17f84",
   "metadata": {},
   "source": [
    "# 2. Email elements extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e692eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b838a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)\n",
    "\n",
    "def split_email_addresses(line):\n",
    "    '''To separate multiple email addresses'''\n",
    "    if line:\n",
    "        addrs = line.split(',')\n",
    "        addrs = list(map(lambda x: x.strip(), addrs))\n",
    "    else:\n",
    "        addrs = None\n",
    "    return addrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95713e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Parse the emails into a list email objects\n",
    "messages = list(map(email.message_from_string, data['message_text']))\n",
    "data.drop(columns=['message_text'], inplace=True)\n",
    "\n",
    "# Get fields from parsed email objects\n",
    "#keys = messages[0].keys()\n",
    "keys=['Message-ID', 'Date', 'From', 'To', 'Subject', 'Cc', 'Bcc','X-From', 'X-To', 'X-cc', 'X-bcc', 'X-Folder', 'X-Origin', 'X-FileName']\n",
    "\n",
    "for key in keys:\n",
    "    data[key] = [doc[key] for doc in messages]\n",
    "    \n",
    "# Parse content from emails\n",
    "data['body'] = list(map(get_text_from_email, messages))\n",
    "# Split multiple email addresses\n",
    "data['From'] = data['From'].map(split_email_addresses)\n",
    "data['To']   = data['To'].map(split_email_addresses)\n",
    "\n",
    "# Extract the root of 'file' as 'user'\n",
    "data['user'] = data['message_id'].map(lambda x:x.split('/')[0])\n",
    "del messages\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c340a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data[['message_id','Date','From','To','Subject','Cc','Bcc','body']]\n",
    "df=df.rename(columns={\"Date\": \"date\", \"From\": \"from\",'To':'to','Subject':'subject','Cc':'cc','Bcc':'bcc'})\n",
    "df.to_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\complete data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947785e",
   "metadata": {},
   "source": [
    "# 2.  Duplicate elimination, HTML extractor and Thread split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72500fb",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d443f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>date</th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>subject</th>\n",
       "      <th>cc</th>\n",
       "      <th>bcc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p\\all_documents\\1</td>\n",
       "      <td>Wed, 13 Dec 2000 18:41:00 -0800 (PST)</td>\n",
       "      <td>['1.11913372.-2@multexinvestornetwork.com']</td>\n",
       "      <td>['pallen@enron.com']</td>\n",
       "      <td>December 14, 2000 - Bear Stearns' predictions ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>In today's Daily Update you'll find free repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p\\all_documents\\10</td>\n",
       "      <td>Wed, 13 Dec 2000 08:35:00 -0800 (PST)</td>\n",
       "      <td>['messenger@ecm.bloomberg.com']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p\\all_documents\\100</td>\n",
       "      <td>Mon, 9 Oct 2000 07:16:00 -0700 (PDT)</td>\n",
       "      <td>['phillip.allen@enron.com']</td>\n",
       "      <td>['keith.holst@enron.com']</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>---------------------- Forwarded by Phillip K ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p\\all_documents\\101</td>\n",
       "      <td>Mon, 9 Oct 2000 07:00:00 -0700 (PDT)</td>\n",
       "      <td>['phillip.allen@enron.com']</td>\n",
       "      <td>['keith.holst@enron.com']</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>---------------------- Forwarded by Phillip K ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p\\all_documents\\102</td>\n",
       "      <td>Thu, 5 Oct 2000 06:26:00 -0700 (PDT)</td>\n",
       "      <td>['phillip.allen@enron.com']</td>\n",
       "      <td>['david.delainey@enron.com']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dave, \\n\\n Here are the names of the west desk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  message_id                                   date  \\\n",
       "0    allen-p\\all_documents\\1  Wed, 13 Dec 2000 18:41:00 -0800 (PST)   \n",
       "1   allen-p\\all_documents\\10  Wed, 13 Dec 2000 08:35:00 -0800 (PST)   \n",
       "2  allen-p\\all_documents\\100   Mon, 9 Oct 2000 07:16:00 -0700 (PDT)   \n",
       "3  allen-p\\all_documents\\101   Mon, 9 Oct 2000 07:00:00 -0700 (PDT)   \n",
       "4  allen-p\\all_documents\\102   Thu, 5 Oct 2000 06:26:00 -0700 (PDT)   \n",
       "\n",
       "                                          from                            to  \\\n",
       "0  ['1.11913372.-2@multexinvestornetwork.com']          ['pallen@enron.com']   \n",
       "1              ['messenger@ecm.bloomberg.com']                           NaN   \n",
       "2                  ['phillip.allen@enron.com']     ['keith.holst@enron.com']   \n",
       "3                  ['phillip.allen@enron.com']     ['keith.holst@enron.com']   \n",
       "4                  ['phillip.allen@enron.com']  ['david.delainey@enron.com']   \n",
       "\n",
       "                                             subject   cc  bcc  \\\n",
       "0  December 14, 2000 - Bear Stearns' predictions ...  NaN  NaN   \n",
       "1                       Bloomberg Power Lines Report  NaN  NaN   \n",
       "2        Consolidated positions: Issues & To Do list  NaN  NaN   \n",
       "3        Consolidated positions: Issues & To Do list  NaN  NaN   \n",
       "4                                                NaN  NaN  NaN   \n",
       "\n",
       "                                                body  \n",
       "0  In today's Daily Update you'll find free repor...  \n",
       "1  Here is today's copy of Bloomberg Power Lines....  \n",
       "2  ---------------------- Forwarded by Phillip K ...  \n",
       "3  ---------------------- Forwarded by Phillip K ...  \n",
       "4  Dave, \\n\\n Here are the names of the west desk...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\complete data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b87298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are interested only on subject and body\n",
    "data.drop(['date','from','to','cc','bcc'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52b0023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(['subject', 'body'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf44f2b7",
   "metadata": {},
   "source": [
    "## HTML cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2acedd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(text):\n",
    "    pattern = \"<html>(.+?)</html>\"\n",
    "    if re.search(pattern, text, re.IGNORECASE + re.DOTALL):\n",
    "        soup = BeautifulSoup(text, 'html5')\n",
    "        for data in soup(['style']):\n",
    "              data.decompose()\n",
    "        soup = BeautifulSoup(soup.get_text(), 'html5')\n",
    "        return soup.get_text()\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e19c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing htmls\n",
    "data['body_html'] = data['body'].map(parse_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f8c6bb",
   "metadata": {},
   "source": [
    "## Thread splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac6129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose no lower case To From Subject\n",
    "def chain_split(a, subject):\n",
    "    # print(a)\n",
    "    text = a\n",
    "    a = a.split('\\n')\n",
    "    end = []\n",
    "    start = []\n",
    "    subs = []\n",
    "    i = 0\n",
    "    if_chain = False\n",
    "    while i < len(a):\n",
    "        if re.search(re.compile(r'forwarded by|original message|--original', re.IGNORECASE),a[i]):\n",
    "            start.append(i)\n",
    "        elif len(start)==len(end) and re.search(r'From:',a[i]):\n",
    "            start.append(i)\n",
    "        elif len(start)==len(end) and re.search(r'To:',a[i]):\n",
    "            start.append(i-3)\n",
    "        elif re.search(r'Subject:',a[i]):\n",
    "            s = i\n",
    "            if len(end)==len(start):\n",
    "                if len(start)!=0:\n",
    "                    subs[-1] = s\n",
    "                    end[-1] = i\n",
    "            else:\n",
    "                subs.append(s)\n",
    "                end.append(i)\n",
    "        i+=1\n",
    "\n",
    "    if len(start)>len(end):\n",
    "        start = start[:len(end)]\n",
    "\n",
    "    emails = []\n",
    "    subjects = []\n",
    "    pre = 0\n",
    "    subjects.append(subject)\n",
    "    for sub in subs: \n",
    "        subjects.append(a[sub].split('Subject:')[-1].strip('\\t'))\n",
    "    for s, e in zip(start, end):\n",
    "        if pre != s:\n",
    "            emails.append(\"\\n\".join(a[pre:s]).strip('\\n'))\n",
    "        else:\n",
    "            emails.append(\" \")\n",
    "        pre = e+1\n",
    "        for i in range(s, e+1):\n",
    "            a[i] = \"\"\n",
    "    if pre < len(a):\n",
    "        emails.append(\"\\n\".join(a[pre:]).strip('\\n'))\n",
    "    else:\n",
    "        emails.append(\" \")\n",
    "\n",
    "    combine = [(len(emails)-i, sub, email) for i, (sub, email) in enumerate(zip(subjects, emails))]\n",
    "    combine.reverse()\n",
    "    a = \"\\n\".join(a)\n",
    "    return len(start), combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a060f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['new_body'] = data.apply(lambda x:chain_split(x['body_html'],x['subject']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b7f4c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['chain_len'] = data['new_body'].map(lambda x:x[0])\n",
    "data['subjects_emails'] = data['new_body'].map(lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe6107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = data[['message_id','subjects_emails']].explode('subjects_emails')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5afa3333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p\\all_documents\\1#1</td>\n",
       "      <td>December 14, 2000 - Bear Stearns' predictions ...</td>\n",
       "      <td>In today's Daily Update you'll find free repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p\\all_documents\\10#1</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p\\all_documents\\100#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p\\all_documents\\100#2</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>Phillip,\\n Below is the issues &amp; to do list as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p\\all_documents\\100#3</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466893</th>\n",
       "      <td>zufferli-j\\sent_items\\98#1</td>\n",
       "      <td>Calgary Analyst/Associate</td>\n",
       "      <td>Analyst\\t\\t\\t\\t\\tRank\\n\\nStephane Brodeur\\t\\t\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466894</th>\n",
       "      <td>zufferli-j\\sent_items\\99#1</td>\n",
       "      <td>ali's essays</td>\n",
       "      <td>Hi John\\n\\n   How was Thanksgiving?  Was th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466895</th>\n",
       "      <td>zufferli-j\\sent_items\\99#2</td>\n",
       "      <td>RE: ali's essays</td>\n",
       "      <td>01:41 PM\\n\\n\\n\\n\\n\\n\\njust...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466896</th>\n",
       "      <td>zufferli-j\\sent_items\\99#3</td>\n",
       "      <td>RE: ali's essays</td>\n",
       "      <td>i don't know about the heart classes.  i'll lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466897</th>\n",
       "      <td>zufferli-j\\sent_items\\99#4</td>\n",
       "      <td>RE: ali's essays</td>\n",
       "      <td>i think the YMCA has a class that is for peopl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>466898 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           email_id  \\\n",
       "0         allen-p\\all_documents\\1#1   \n",
       "1        allen-p\\all_documents\\10#1   \n",
       "2       allen-p\\all_documents\\100#1   \n",
       "3       allen-p\\all_documents\\100#2   \n",
       "4       allen-p\\all_documents\\100#3   \n",
       "...                             ...   \n",
       "466893   zufferli-j\\sent_items\\98#1   \n",
       "466894   zufferli-j\\sent_items\\99#1   \n",
       "466895   zufferli-j\\sent_items\\99#2   \n",
       "466896   zufferli-j\\sent_items\\99#3   \n",
       "466897   zufferli-j\\sent_items\\99#4   \n",
       "\n",
       "                                                  subject  \\\n",
       "0       December 14, 2000 - Bear Stearns' predictions ...   \n",
       "1                            Bloomberg Power Lines Report   \n",
       "2             Consolidated positions: Issues & To Do list   \n",
       "3             Consolidated positions: Issues & To Do list   \n",
       "4             Consolidated positions: Issues & To Do list   \n",
       "...                                                   ...   \n",
       "466893                          Calgary Analyst/Associate   \n",
       "466894                                       ali's essays   \n",
       "466895                                   RE: ali's essays   \n",
       "466896                                   RE: ali's essays   \n",
       "466897                                   RE: ali's essays   \n",
       "\n",
       "                                                    email  \n",
       "0       In today's Daily Update you'll find free repor...  \n",
       "1       Here is today's copy of Bloomberg Power Lines....  \n",
       "2       From our initial set of meetings with the trad...  \n",
       "3       Phillip,\\n Below is the issues & to do list as...  \n",
       "4                                                          \n",
       "...                                                   ...  \n",
       "466893  Analyst\\t\\t\\t\\t\\tRank\\n\\nStephane Brodeur\\t\\t\\...  \n",
       "466894     Hi John\\n\\n   How was Thanksgiving?  Was th...  \n",
       "466895                      01:41 PM\\n\\n\\n\\n\\n\\n\\njust...  \n",
       "466896  i don't know about the heart classes.  i'll lo...  \n",
       "466897  i think the YMCA has a class that is for peopl...  \n",
       "\n",
       "[466898 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.reset_index(inplace = True)\n",
    "df_new.drop([\"index\"], axis= 1, inplace = True)\n",
    "# df_new = df_new[df_new['subjects_emails'].notna()]\n",
    "df_new['idx'] = df_new['subjects_emails'].map(lambda x:x[0])\n",
    "df_new['subject'] = df_new['subjects_emails'].map(lambda x:x[1])\n",
    "df_new['email'] = df_new['subjects_emails'].map(lambda x:x[2])\n",
    "# df_new = df_new[(df_new['email']!=' ') & (df_new['subject']!=' ')]\n",
    "df_new['email_id'] = df_new.apply(lambda x: x['message_id']+'#'+str(x['idx']), axis = 1)\n",
    "df_new = df_new[[\"email_id\", \"subject\", \"email\"]]\n",
    "df_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a38ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\Data splitted.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acda54eb",
   "metadata": {},
   "source": [
    "# 3. disclaimers/signatures/etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1071b4b",
   "metadata": {},
   "source": [
    "Can avoid re-run Nicolo's code by loading directly \"clean_emails_Nicolo_2.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9e0d7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\Data splitted.csv')\n",
    "nicolo=pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\clean_emails_Nicolo_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5a8b521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['email'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aea82f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#df=pd.read_csv('complete_data.csv')\n",
    "#df=pd.read_csv('split_threads (1).csv')\n",
    "# stopped=pd.read_csv('clean_emails_Nicolo.csv')\n",
    "from names_dataset import NameDataset, NameWrapper\n",
    "names=NameDataset()\n",
    "def detect_name(string, r = 5000, c = 3):\n",
    "    '''\n",
    "    Function that returns True if the provided string contains a name (for US present in top 3 countries)\n",
    "    r: Minimum position in the ranking of popular names/last names in the US\n",
    "    c: Minimum ranking of popularity of the word as a name/last name in the US\n",
    "    '''\n",
    "    words = string.split(\" \")\n",
    "    l = False\n",
    "    for w in words:\n",
    "\n",
    "        x = names.search(w)['first_name']\n",
    "        if x is None:# word not found in names dictionary\n",
    "            f = False\n",
    "        elif x['rank']['United States'] is None:# Name no frequent in US\n",
    "            f = False\n",
    "        elif x['rank']['United States'] > r:# Name unfrequent in US \n",
    "            f = False\n",
    "        else:\n",
    "            y = x['country']\n",
    "            countries_first_name = list({k: v for k, v in sorted(y.items(), key=lambda item: item[1], reverse = True)}.keys())[0:c]\n",
    "            f = 'United States' in countries_first_name\n",
    "        \n",
    "        x = names.search(w)['last_name']\n",
    "        if x is None:# word not found in names dictionary\n",
    "            p = False\n",
    "        elif x['rank']['United States'] is None:# Last name no frequent in US\n",
    "            p = False\n",
    "        elif x['rank']['United States'] > r:# Last name unfrequent in US \n",
    "            p = False\n",
    "        else:\n",
    "            y = x['country']\n",
    "            countries_last_name = list({k: v for k, v in sorted(y.items(), key=lambda item: item[1], reverse = True)}.keys())[0:c]\n",
    "            p = 'United States' in countries_last_name     \n",
    "        \n",
    "        l = l | ( f | p )\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c596899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "450307it [44:24, 169.03it/s] \n"
     ]
    }
   ],
   "source": [
    "salutations=[\n",
    "'My Best',\n",
    "'My best to you',\n",
    "'Have a nice day',\n",
    "'Hope your day is going well',\n",
    "'Faithfully',\n",
    "'All Best' ,\n",
    "'All the best',\n",
    "'Best Wishes',\n",
    "'Bests',\n",
    "'Best Regards',\n",
    "'Regards',\n",
    "'Rgds',\n",
    "'Warm Regards',\n",
    "'Warmest Regards',\n",
    "'Warmest',\n",
    "'Warmly',\n",
    "'Take care',\n",
    "'Thanks',\n",
    "'Thanks so much',\n",
    "'Thanks!',\n",
    "'Thank you',\n",
    "'Thank you!',\n",
    "'Thanks a bunch',\n",
    "'Many thanks',\n",
    "'Thanks for your consideration',\n",
    "'Thx',\n",
    "'Hope this helps',\n",
    "'Looking forward',\n",
    "'Rushing',\n",
    "'In haste',\n",
    "'Be well',\n",
    "'Peace',\n",
    "'Yours Truly',\n",
    "'Yours',\n",
    "'Very Truly Yours',\n",
    "'Sincerely',\n",
    "'Sincerely Yours',\n",
    "'Cheers!',\n",
    "'faithfully'\n",
    "'Ciao',\n",
    "'Love',\n",
    "'Lots of love',\n",
    "'Hugs',\n",
    "'High five from down low',\n",
    "'Take it easy bro',\n",
    "'See you around',\n",
    "'Have a wonderful bountiful lustful day',\n",
    "'Sent from my iPhone',\n",
    "'Sent from iOS']\n",
    "#list of common salutations\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "#function that tells us how similar two strings are similar\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "c=False\n",
    "counter=0\n",
    "rest=[]\n",
    "words=[]\n",
    "em=[]\n",
    "se=[]\n",
    "way=[]\n",
    "prev=[]\n",
    "indexes=[]\n",
    "dete=[]\n",
    "for index,row in tqdm(df.iterrows()):\n",
    "    #split the body in line\n",
    "    body=row['email'].split('\\n')\n",
    "    #check if the body is at least three (could be avoided in this case)\n",
    "    if len(body)>3:\n",
    "        #for line in the body \n",
    "        for j,line in enumerate(body):\n",
    "            #if we are in the bottom half of the email we go forth. Assumption: salutation are in the bottom half\n",
    "            if j>(1.5*len(body))/3:\n",
    "                #if the line is less than 4 items, (probably a salutation) and the previous line is empty\n",
    "                #and body[j-1]=='':\n",
    "                if len(line.split())<=4:\n",
    "                    s = ''.join(filter(str.isalnum, line))\n",
    "                    for item in salutations:\n",
    "                        #we wliminate anything that is punctuation or not letters,/numbers\n",
    "                        #we get the similarity for each item in the salutation list. If we get good score we go on\n",
    "                        if similar(s.lower(),item.lower())>0.65 and row['email_id'] not in rest:\n",
    "                            #code for checking any len(3) salutation\n",
    "                            #let's count how many email have this salutatiion\n",
    "                            counter+=1\n",
    "                            em.append(row['email'])\n",
    "                            rest.append(row['email_id'])\n",
    "                            indexes.append(j)\n",
    "                            se.append(s)\n",
    "                            way.append('salutation')\n",
    "                            dete.append(item)\n",
    "                            #let's get the word that triggered our salutation to compare\n",
    "                            words.append(s.lower()+'--------->'+item.lower())\n",
    "                            #Early stopping system to avoid  (change index for early stopping)\n",
    "                            if counter==100000000 or index>5111000:\n",
    "                                c=True\n",
    "                    for item_word in line.split():\n",
    "                        if item_word[0].isupper() and item_word[1:].islower():\n",
    "                            try:\n",
    "                                if detect_name(item_word) and row['email_id'] not in rest:\n",
    "                                    counter+=1\n",
    "                                    em.append(row['email'])\n",
    "                                    rest.append(row['email_id'])\n",
    "                                    indexes.append(j)\n",
    "                                    se.append(s)\n",
    "                                    dete.append(item_word)\n",
    "                                    way.append('name search')\n",
    "                            except:\n",
    "                                #print('error')\n",
    "                                continue\n",
    "        if c==True:\n",
    "            break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "106ac7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clean emails\n",
    "cleaned_em=[]\n",
    "\n",
    "for i in range(len(em)):\n",
    "    cleaned_em.append(' '.join(em[i].split(\"\\n\")[:indexes[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "362a5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "nicolo= {'email_id':rest,'email':cleaned_em}\n",
    "nicolo= pd.DataFrame(nicolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9f06b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nicolo=pd.merge(nicolo, df[['email_id','subject']],how='left')\n",
    "nicolo=nicolo[['email_id','subject','email']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7f0fac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean.to_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\clean_emails_Nicolo_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "6745d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.email_id.isin(nicolo.email_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6cb58eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,nicolo.iloc[:,1:4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "34854326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\Data splitted_no_disc.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85120c",
   "metadata": {},
   "source": [
    "# 4. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efdfbfd",
   "metadata": {},
   "source": [
    "## Cleaning of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02807ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'../data/Data splitted_no_disc.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8eec791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450307, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adc98da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p\\all_documents\\10#1</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p\\all_documents\\100#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allen-p\\all_documents\\100#3</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p\\all_documents\\101#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allen-p\\all_documents\\101#3</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      email_id                                       subject  \\\n",
       "0   allen-p\\all_documents\\10#1                  Bloomberg Power Lines Report   \n",
       "1  allen-p\\all_documents\\100#1   Consolidated positions: Issues & To Do list   \n",
       "2  allen-p\\all_documents\\100#3   Consolidated positions: Issues & To Do list   \n",
       "3  allen-p\\all_documents\\101#1   Consolidated positions: Issues & To Do list   \n",
       "4  allen-p\\all_documents\\101#3   Consolidated positions: Issues & To Do list   \n",
       "\n",
       "                                               email  \n",
       "0  Here is today's copy of Bloomberg Power Lines....  \n",
       "1  From our initial set of meetings with the trad...  \n",
       "2                                                     \n",
       "3  From our initial set of meetings with the trad...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06bf89ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['email_id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d154bc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are they valid maskers?\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# proposed maskers\n",
    "urlmasker = 'urlmasker'\n",
    "imagemasker = 'imagemasker'\n",
    "namemasker = 'namemasker'\n",
    "\n",
    "# validation of maskers\n",
    "print('Are they valid maskers?')\n",
    "print(~df['email'].str.contains(r'\\b' + urlmasker + r'\\b\\s*', '', regex=True).any())\n",
    "print(~df['email'].str.contains(r'\\b' + imagemasker + r'\\b\\s*', '', regex=True).any())\n",
    "print(~df['email'].str.contains(r'\\b' + namemasker + r'\\b\\s*', '', regex=True).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19aedaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary objects\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cdf0d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 243.095 secs\n"
     ]
    }
   ],
   "source": [
    "# data cleaning\n",
    "t0 = time()\n",
    "\n",
    "df['bow'] = df['email'].str.lower() # lower case\n",
    "\n",
    "# maskings\n",
    "df['bow'] = df['bow'].str.replace(r'http\\S+', urlmasker, regex=True) # URLs masking\n",
    "df['bow'] = df['bow'].str.replace('[image]', ' '+imagemasker+' ', regex=False) # images masking\n",
    "df['bow'] = df['bow'].str.replace('<.*?>', '', regex=True) # remove tags (there were tags present)\n",
    "\n",
    "# remove punctuation\n",
    "df['embeddings'] = df['bow']\n",
    "df['bow'] = df['bow'].str.replace(r'[^\\w\\s]', ' ', regex=True)\n",
    "\n",
    "# remove underscore words\n",
    "df['bow'] = df['bow'].str.replace(r'\\b[_]+\\b', '', regex=True)\n",
    "df['embeddings'] = df['embeddings'].str.replace(r'\\b[_]+\\b', '', regex=True)\n",
    "\n",
    "# remove numeric words\n",
    "df['bow'] = df['bow'].str.replace(r'\\b[0-9]+\\b\\s*', '', regex=True)\n",
    "df['embeddings'] = df['embeddings'].str.replace(r'\\b[0-9]+\\b\\s*', '', regex=True)\n",
    "\n",
    "# lemmatize\n",
    "lemmatize_words = lambda wl : [lemmatizer.lemmatize(w) for w in wl]\n",
    "df['bow'] = df['bow'].str.split().apply(lemmatize_words).str.join(' ')\n",
    "df['embeddings'] = df['embeddings'].str.split().apply(lemmatize_words).str.join(' ')\n",
    "\n",
    "# remove stopwords\n",
    "remove_stopwords = lambda wl : [w for w in wl if not w in stop_words]\n",
    "df['bow'] = df['bow'].str.split().apply(remove_stopwords).str.join(' ')\n",
    "df['embeddings'] = df['embeddings'].str.split().apply(remove_stopwords).str.join(' ')\n",
    "\n",
    "# remove extra whitespaces\n",
    "df['bow'] = df['bow'].str.split().str.join(' ')\n",
    "df['embeddings'] = df['embeddings'].str.split().str.join(' ')\n",
    "\n",
    "# remove short words\n",
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 50\n",
    "valid_length = lambda wl : [w for w in wl if len(w) >= MIN_LENGTH and len(w) <= MAX_LENGTH]\n",
    "df['bow'] = df['bow'].str.split().apply(valid_length).str.join(' ')\n",
    "df['embeddings'] = df['embeddings'].str.split().apply(valid_length).str.join(' ')\n",
    "\n",
    "df = df[df['bow'] != ''] # remove samples with no body text\n",
    "\n",
    "print(f'Done in {time()-t0:.3f} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cb56471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "def min_len(wl):\n",
    "    if len(wl) > 0: return min([len(w) for w in wl])\n",
    "    else: return np.inf\n",
    "\n",
    "aux = df['bow'].str.split().apply(min_len)\n",
    "aux.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aefbbd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433017, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0b70a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>email_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>bow</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p\\all_documents\\10#1</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "      <td>today copy bloomberg power line adobe acrobat ...</td>\n",
       "      <td>today's copy bloomberg power lines. adobe acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p\\all_documents\\100#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "      <td>initial set meeting trader regarding consolida...</td>\n",
       "      <td>initial set meeting trader regarding consolida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allen-p\\all_documents\\101#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "      <td>initial set meeting trader regarding consolida...</td>\n",
       "      <td>initial set meeting trader regarding consolida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>allen-p\\all_documents\\108#1</td>\n",
       "      <td>Re: Not business related..</td>\n",
       "      <td>I think Fletch has a good CPA.  I am still doi...</td>\n",
       "      <td>think fletch good cpa still</td>\n",
       "      <td>think fletch good cpa. still own.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>allen-p\\all_documents\\11#1</td>\n",
       "      <td>Special report coming from NewsData</td>\n",
       "      <td>Our Sacramento correspondent just exited a new...</td>\n",
       "      <td>sacramento correspondent exited news conferenc...</td>\n",
       "      <td>sacramento correspondent exited news conferenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      email_id                                       subject  \\\n",
       "0   allen-p\\all_documents\\10#1                  Bloomberg Power Lines Report   \n",
       "1  allen-p\\all_documents\\100#1   Consolidated positions: Issues & To Do list   \n",
       "3  allen-p\\all_documents\\101#1   Consolidated positions: Issues & To Do list   \n",
       "8  allen-p\\all_documents\\108#1                    Re: Not business related..   \n",
       "9   allen-p\\all_documents\\11#1           Special report coming from NewsData   \n",
       "\n",
       "                                               email  \\\n",
       "0  Here is today's copy of Bloomberg Power Lines....   \n",
       "1  From our initial set of meetings with the trad...   \n",
       "3  From our initial set of meetings with the trad...   \n",
       "8  I think Fletch has a good CPA.  I am still doi...   \n",
       "9  Our Sacramento correspondent just exited a new...   \n",
       "\n",
       "                                                 bow  \\\n",
       "0  today copy bloomberg power line adobe acrobat ...   \n",
       "1  initial set meeting trader regarding consolida...   \n",
       "3  initial set meeting trader regarding consolida...   \n",
       "8                        think fletch good cpa still   \n",
       "9  sacramento correspondent exited news conferenc...   \n",
       "\n",
       "                                          embeddings  \n",
       "0  today's copy bloomberg power lines. adobe acro...  \n",
       "1  initial set meeting trader regarding consolida...  \n",
       "3  initial set meeting trader regarding consolida...  \n",
       "8                  think fletch good cpa. still own.  \n",
       "9  sacramento correspondent exited news conferenc...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b805721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 10.250 secs\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "MIN_DF = 10\n",
    "MAX_DF = 0.5\n",
    "MAX_FEATURES = 30_000\n",
    "vectorizer = CountVectorizer(max_df=MAX_DF, min_df=MIN_DF, max_features=MAX_FEATURES)\n",
    "X = vectorizer.fit_transform(df['bow'])\n",
    "\n",
    "print(f'Done in {time()-t0:.3f} secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04475982",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vocab = list(vectorizer.get_feature_names_out())\n",
    "counts_vocab = X.sum(axis=0).tolist()[0]\n",
    "df_freq = pd.DataFrame(zip(words_vocab, counts_vocab), columns=['term', 'frequency'])\n",
    "df_freq = df_freq.sort_values(by='frequency', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1fa3d708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74ced0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAegklEQVR4nO3de3RV5Z3/8fc390BIQkgCIQmEmwhyN9y8oKhVpKXai1V0rG2ttNPRTi+zZuzqb820v67215nftJ3aOrZYrbb1h1LHe61iBUUUgSAXCdeAQALkCoSAJOTy/P44J3gSEkzISfbZJ5/XWlk55znn7PN9uuXTJ89+9t7mnENERKJLjNcFiIhI+CncRUSikMJdRCQKKdxFRKKQwl1EJArFeV0AQGZmpisoKPC6DBERX9m4cWO1cy6ro9ciItwLCgooKiryugwREV8xswOdvaZpGRGRKORpuJvZIjNbWltb62UZIiJRx9Nwd8696JxbkpaW5mUZIiJRR9MyIiJRSOEuIhKFFO4iIlFI4S4iEoV8He5r99bw8xW7aGnRZYtFREL5Otw37D/KAytLULSLiLTl63CPscDvFt1wRESkDV+fxGQWSHeFu4hIW74+iSkmGO7KdhGRtjQtIyIShXwe7q3TMh4XIiISYXwd7qaRu4hIh3we7ppzFxHpiK/DvXXO3SndRUTa8Hm4a85dRKQjPg/3wG/NuYuItOXrcNdJTCIiHfN1uOskJhGRjvk83AO/NXIXEWkr7OFuZleb2Vtm9hszuzrc22/7XYHfOqAqItJWl8LdzB41s0oz29aufYGZ7TKzEjO7P9jsgJNAElAW3nLPqSvwhRq5i4i00dWR+2PAgtAGM4sFHgRuBCYCi81sIvCWc+5G4F+AH4av1HNpzl1EpGNdCnfn3GrgaLvmWUCJc26fc+4M8CRwk3OuJfj6MSCxs22a2RIzKzKzoqqqqgsoXXPuIiKd6cmcey5QGvK8DMg1s8+a2W+BPwK/7uzDzrmlzrlC51xhVlbWBRWgk5hERDoW14PPWgdtzjn3DPBMlzZgtghYNHbs2AsrIFhBs9JdRKSNnozcy4D8kOd5wOHubKCnN+tIiA2U39jc8jHvFBHpX3oS7huAcWY2yswSgNuAF8JTVtckxgfKb2hSuIuIhOrqUshlwFpgvJmVmdndzrkm4F7gVWAHsNw5V9ydL+/pPVQTYmMBOKNwFxFpo0tz7s65xZ20vwy8fKFf7px7EXixsLDwngv5fEJc4P+bFO4iIm35+vIDia3h3tzscSUiIpHF03Dv8bRMMNwbGjVyFxEJ5Wm493i1zNmRu8JdRCSUr6dlWpdCarWMiEhbvp6WaV0KqQOqIiJt+XpaJjG4FFIjdxGRtvw9LaOlkCIiHVK4i4hEIV/PucfGGHExRkOT1rmLiITy9Zw7BEbvGrmLiLTl62kZCJylqnXuIiJt+T7cE+JidIaqiEg7vg/3lMQ46hoavS5DRCSi+PqAKsCQgYkcPXUmjFWJiPif7w+oDkqKo66+KYxViYj4n/+nZZLiONmgcBcRCeX7cB+UFMdJjdxFRNrwfbinJydw/HQjLS3O61JERCKG78N9aGoizS2O6lMNXpciIhIxfL9aZmhqEgAVtQp3EZFWvl8tMywtEO5Hak+HqywREd/z/bTMsNaR+4l6jysREYkcvg/3ISmJxMYY5Qp3EZGzfB/usTFG9qBEyjXnLiJylu/DHQIHVTUtIyLykagI92GpSZqWEREJER3hnpZERa3CXUSkVVSE+9DUJOoamjila8yIiABRcBITwLC0RABNzYiIBPn+JCaAvMEDADhQcyocZYmI+F5UTMuMy04BYG+lwl1EBKIk3NMHJJCZksjO8jqvSxERiQhREe4A0/LT2HTwmNdliIhEhKgJ95kFGeyrPkVVnc5UFRGJmnCfmp8OwPYjJ7wtREQkAkRNuE/ISSXGoGj/Ua9LERHxXNSEe1pyPJPz0nlrT7XXpYiIeC5qwh3gqouy2Fp2nLr6Rq9LERHxVFSF+8yCwbQ42HTwuNeliIh4KqrCffqIwcTGGBs07y4i/VyvhLuZDTSzjWb2qd7YfmdSEuOYmJOqcBeRfq9L4W5mj5pZpZlta9e+wMx2mVmJmd0f8tK/AMvDWWhXzSzIYNPB49Q3Nnvx9SIiEaGrI/fHgAWhDWYWCzwI3AhMBBab2UQzuw7YDlSEsc4um3dRJg1NLazdW+PF14uIRIS4rrzJObfazAraNc8CSpxz+wDM7EngJiAFGEgg8E+b2cvOuZb22zSzJcASgBEjRlxwB9qbM3oIqUlxPP1eGfMvzg7bdkVE/KRL4d6JXKA05HkZMNs5dy+AmX0JqO4o2AGcc0uBpQCFhYWuB3W0kRQfy2dn5LFs/UE+PNPEgISedFFExJ96ckDVOmg7G9LOuceccy+ddwNhullHe9dPHEpDUwurd+uEJhHpn3oS7mVAfsjzPOBwdzYQrpt1tDdzVAZpyfGs2F4e1u2KiPhFT8J9AzDOzEaZWQJwG/BCeMrqmfjYGK4cl8nq3dU0t4RtxkdExDe6uhRyGbAWGG9mZWZ2t3OuCbgXeBXYASx3zhV358t7a1oGYMGkYVSfbNCqGRHpl7oU7s65xc65HOdcvHMuzzn3SLD9ZefcRc65Mc65H3f3y3trWgbguglDGZgQy1/ePxL2bYuIRLqouvxAqKT4WK4cl8XfdlTQ1Nzhgh0Rkajlabj35rQMwGdm5FJV18DKnZW9sn0RkUjlabj35rQMwDUXZzM0NZE/rTvYK9sXEYlUUTstA4FVMzdPy+WdkmqqT+reqiLSf0T1tAzAoqnDaWpxvL7Dk0vdiIh4IqqnZQAuGZ7KqMyBPL+5W+dXiYj4WlRPywCYGYum5PDuvhoO1nzodTkiIn0i6sMd4I45I4mNMX63Zp/XpYiI9Imon3MHGJqaxGem57K8qJQaHVgVkX4g6ufcWy2ZN5r6xhYeX3ug179LRMRr/WJaBmBs9iCumzCUx97+gJMNTV6XIyLSq/pNuAN889qx1DU0sfTNvV6XIiLSq/pVuE/JS+f6iUN5ZM0HHKk97XU5IiK9pl8cUA31/YUTaWx2PPB6SZ99p4hIX+s3B1RbjRgygM9dmsfTG0s5dFyjdxGJTv1qWqbVvdeMxcz48V+2e12KiEiv6JfhnpuezH3zx/Ly++Ws2qXLAYtI9OmX4Q6w5KrRjM1O4V+f38bpM81elyMiElb9NtwT42L50U2TKD16mv94dafX5YiIhFW/Wy0Tau6YIdw1dyS/f3s/W8uOe1KDiEhv6HerZdr7pxvGkzEwgZ+8vAPnnGd1iIiEU7+dlmk1KCmeb3/iIt7dd5RXi3VDDxGJDv0+3AFum5nPuOwUfvBCMbWnG70uR0SkxxTuBO61+rMvTKXqZAM/eklr30XE/xTuQVPy0vn6VaN5emMZz24q87ocEZEeUbiH+NZ1FzGzYDD3/8/77K6o87ocEZELpnAPER8bw4N3zCAlMY5vLtukk5tExLcU7u1kD0riP78wlV0Vdfzvl4q9LkdE5IL065OYOjN/fDZfmzeGZetLeeY9zb+LiP/0+5OYOvPd6y9i9qgMvv/sNvZXn/K6HBGRbtG0TCfiY2P4xa3TiI81vr18M43NLV6XJCLSZQr38xiensyPbp7EpoPH+dXre7wuR0SkyxTuH+Omabl8bkYev15Vwtsl1V6XIyLSJQr3LvjhTZcwJiuFr/9xI3u0/l1EfEDh3gUpiXE8/pVZJCXEcucj69lVroAXkcimcO+i4enJ/OErs2hxjjsfWaeAF5GIpnDvhgk5qfzpq7NxwN2Pb6Cyrt7rkkREOqRw76aLhg7i4S8WUnPyDHc8vI66el0iWEQij8L9AkzLT+d3dxWyr/oUX328iBMKeBGJMAr3C3T52Ex+/oWpbDxwjFseWkv1yQavSxIROSvs4W5mE8zsN2b2tJn9fbi3H0lumpbLY1+exQc1p7j94Xep/VAjeBGJDF0KdzN71MwqzWxbu/YFZrbLzErM7H4A59wO59zXgS8AheEvObJcMS6TR+4q5IPqU9z28LscPXXG65JERLo8cn8MWBDaYGaxwIPAjcBEYLGZTQy+9mlgDfB62CqNYFeOy2LpFwvZW3mSJX8oor5R14EXEW91Kdydc6uBo+2aZwElzrl9zrkzwJPATcH3v+Ccuwy4o7NtmtkSMysys6KqqqoLqz6CzB+fzf+9ZQobDx7ji4+u1zJJEfFUT+bcc4HSkOdlQK6ZXW1mD5jZb4GXO/uwc26pc67QOVeYlZXVgzIix03TcvmvW6expfQ4C3+5hp3lJ7wuSUT6qZ6Eu3XQ5pxzbzjnvumc+5pz7sEebN+XbpqWy/P3Xk5sDNz+8DqK9rf/g0dEpPf1JNzLgPyQ53nA4e5sIFLvxNRTFw9L5cklcxmUFMftD69jgwJeRPpYT8J9AzDOzEaZWQJwG/BCdzYQyXdi6qlRmQN5cskcctKTWLz0Xf707gGcc16XJSL9RFeXQi4D1gLjzazMzO52zjUB9wKvAjuA5c65bt1ROlpH7q1y0pJ5/h8uZ+6YIfyv57Zx77JNWiopIn3CImE0WVhY6IqKirwuo9e0tDgeXFXCL1/fQ2ZKIr+6fTozCzK8LktEfM7MNjrnOjyfSJcf6AMxMcZ9147j2W9cTmJ8DHf8bh3PvFfmdVkiEsU8Dfdon5Zpb3JeGs/8/WVMz0/nO8u38JOXd3CmSTfeFpHw8zTco/mAameGpCTyh7tncfvsESxdvY9bfruW0qMfel2WiEQZTct4IDEulp98ZjIP3j6DPRV1LPr1Gp7dpGkaEQkfhbuHPjklh5fuu4LRmQP59lNb+Oent3CqocnrskQkCmjO3WOjs1JY/rW5fP2qMSwvKuP6X6zmhS2HtSZeRHpEc+4RIC42hvtvvJj/d89s0pLj+eayTSz540ZdH15ELpimZSLIZWMyefG+K/jejRezcmcln/zVW2w8oEsXiEj3KdwjTGyM8bWrxvDkkjk0tzg+99BavvPUZt3GT0S6RXPuEWpmQQavfecqvn7VGJ7bfIhrf/YmD64qoaFJNwIRkY+nyw/4wPbDJ/jpKztZvbuKyblpPLB4OqMyB3pdloh4TJcf8LmJw1P5w1dm8avF09lfc4rrf/Em//7KTurqdcBVRDqmcPeRRVOH8+q35rFgUg4PvbGXhQ+8xcqdFV6XJSIRSHPuPjM8PZlfLZ7Osnvm0NICX3msiNsffpfNpce9Lk1EIojm3H2soamZR9fs5+G39nHswzPccmke371+PENTk7wuTUT6wPnm3BXuUaCuvpGfrdjN42v3kxAbw51zRvKN+WPJGJjgdWki0osU7v1ESWUdD7xewgtbDpOWHM89V47izrkFpCXHe12aiPQChXs/s+1QLT98sZgN+4+RmhTHly4fxR2zR2i6RiTKKNz7qbV7a3jozb2s3l1FQlwMd18xii9dVqCQF4kSCvd+bld5Hb94bTevFJcTG2NcNyGbL18+ijmjh3hdmoj0QMSGu5ktAhaNHTv2nj179nhWR39RUnmS5UWlLFt/kLr6JibnpvHVK0dx46QcEuJ0yoOI30RsuLfSyL1v1dU38tSGUh57Zz9lx06TmZLIly8v4O9mjyRtgA6+iviFwl061NLiWLWrkt+/vZ81JdUkx8fy6anDufeaseRnDPC6PBH5GAp3+VjFh2t5dM1+ntt8CAM+f2ked8weyaTcVMzM6/JEpAMKd+mygzUf8pvVe/lzUSmNzY7pI9K5d/5Yrrk4WyEvEmEU7tJtNScbeHbTIR5Z8wFHauuZkJPKnXNG8vlL83TwVSRCKNzlgp1pamF5USm/f/sD9ladYlBiHJ+7NI87545kTFaK1+WJ9GsKd+kx5xyvFpfz9MYyVu2qosU5bpg4jDvnjmTu6CHExGjKRqSvnS/c4/q6GPEnM2PBpBwWTMqhvLaeR9bs44l1B3mluJzEuBjmjB7C5y7NY9GUHM3Ni0QAncQkF+xEfSMriitYs6eKF7ceobnFkZYcz22z8vm72SO1nFKkl2laRnpdfWMzy4tKeeLdg+yqqANgcm4at87M59aZ+cTH6iCsSLgp3KVPvV9Wy1NFB3n2vUOcOtNMYlwMt83M5555o8kbrNG8SLgo3MUTTc0tPL2xjD+tO8C2QycAmJqXxsLJOdw6M5/0AbqZiEhPKNzFc1vLjvPEuwd5aethTp1pBmDO6Aw+NWU4N00bzqAkXdNGpLsU7hIxnHOs3lPN8qJS/rL1yNn26SPSuXlaLjdPz9Wdo0S6SOEuEam5xfHKtnJWbC/npeBqG4DZozJYMGkYN0/LZbDuAyvSKYW7RLzmFsdr28t5YcthXn6//Gz7jBHpfGZGHjdr6kbkHAp38ZWm5hbe3F3F85sP88KWw2fbLx42iJun53JrYb5G9CIo3MXHmlsClz14bXsFf3n/CGeaWoDAGvprJ2SzaOpwXeNG+i2Fu0QF5xwrd1byzHuHWL27irqGJgDyM5K5fuIwPjsjl0uGp3lcpUjf6fNwN7ObgU8C2cCDzrkV53u/wl26yznH1rJanioqZUVxBdUnGwBIjIth/vhsrpmQzaenDicpPtbjSkV6T1jC3cweBT4FVDrnJoW0LwB+CcQCv3PO/TTktcHAfzrn7j7fthXu0lMllXU8u+kQ7+ytYdPB42fbLx42iCvHZfLZGXlMyEn1rkCRXhCucJ8HnAT+0BruZhYL7AY+AZQBG4DFzrntwdd/BjzhnHvvfNtWuEs41Tc289LWI7yxq5I3d300fQMwqyCDuWOGcP0lQ5kwLFWXKhZfC9u0jJkVAC+FhPtc4AfOuRuCz78XfOtPgz+vOef+9nHbVbhLb9p++AQvbT3Miu0VlFSebPPa/PFZ3DgphxsuGUbaAC21FH/pzXD/PLDAOffV4PM7gdkERvN3ERjJb3bO/aaDbS0BlgCMGDHi0gMHDnSnTyIXpKm5hdV7qli5s5IVxRVU1jWcfS1vcDLzLspi8cwRujG4+EJvhvstwA3twn2Wc+6+7hSokbt4pfJEPX/dVs7rOytZvbvqbHt8rFE4MoOrxmfxqSk5upqlRKTevBNTGZAf8jwPONzJezsqrPVmHT0sQ+TCZKcmcddlBdx1WQEA7+yt5rlNh9iw/xhr99Wwdl8NP/3rTlKT4rjyoizumDWCwoIM3SRcIl5PR+5xBKZgrgUOEZiGud05V9ydIjRyl0hUV9/I6zsqWbG9vM0lEQBGZw5kwvBU5owewtUXZemuU+KJcK2WWQZcDWQCFcC/OeceMbOFwH8RWAr5qHPux90tUOEuflB8uJaVOyp5Z28NW8qO82Hw0sUQWF8/syCDqflpXDthKBNzUrXGXnpdxJ6hqnuoip/Vnm7kb9sr2FR6jHf3HT1nJc5FQ1OYNy6Lq8ZncdmYTGK17FLCLGLDvZVG7hINPjzTxKaDx3lzdxUbDxxj44FjbV4fm53C3NFDmH9xFleMzdK8vfRYxIa7Ru4S7XYcOcGfi8pYv7/m7K0GWyXFxzBvXBbTRwymsGAwU/PSFfjSLREb7q00cpf+oKm5hT2VJ3ljVxXvHTzGgZpT7K5oO5UzLDWJCTmD+OSU4Vw2ZgjDUpN0Fq10qjeXQopIF8XFxjAhJ7XNNW6OnTrD3qqTrPvgKKt2VvJB9SlW7api1a6P1tzPGZ1B3uABLJw8jMvGZOpArXSJRu4iEebw8dNs2H+U4sMnWFFczv6aD9u8nhgXw/WXDGN05kCuuTibMdkppCRqnNYfRey0jObcRT5e63TOq8XllB49zYri8jYXQ4PAypxJuWlMGp7Gwsk5pA+I1wi/H4jYcG+lkbtI95xqaGL9/qO8vaeasmOneaW4/Jz3jM1OYUJOKrfNzGd4ejKjMgd6UKn0JoW7SJRrbG7hdGMzq3ZWUl5bz7L1BzlSW09D8LaEEDirFoNbLs1nyMAEFk7J0XSOzyncRfoh5xybSo9TUnGSv+0I3K3qvZAbmUBg/n5KXhozCzIYP2wQ88ZlERNjpCXr8sd+ELHhrjl3kb5Ve7qR+sZmXtxymKqTDTz+zn6amh1NLW1zYOSQAcwqyCA1OZ47Zo8gIS6G3PRkXQY5wkRsuLfSyF3EW0dqT7OiuALnHI+8/QHNzY7DtfVt3jNyyAAmDU8jNTnu7FU0c9OTGZSkUb5XFO4i0m31jc28vqOS043N/PcbJcSYnXP9HIAbJw0DYEJOKgsnBx7nDR6g1Tp9QOEuImFR39jMW3uqaWpu4fWdlWwtOw5wzpm2AJ+dngvAxOGpfHracABSEuMYkKCDuOGicBeRXlVZV8+6fUcBeG7TIXZX1gFQevT0Oe+9c85IkhNiiY817r5iNBkDE/q01mgSseGuA6oi0a3iRD1/21GBc/B+WS3PbT6EGTQ0tdAaPfGxhnNwSW4ai2cGbuw2NC2J+eOzPazcHyI23Ftp5C7S/yxbf5DSo4FLKyxdve+cFTujMgcyKCkwhTNjxGBunfnRHT2HpyWTNkAHchXuIhLRGptbqD7ZAMCR2np+vbKE1mwKvYhaqHuuHHX28bT8wXxySk7vFxphFO4i4luVdfW8F3Ljk5e2HmHlzsqzz1tvdzg0NfFs28yCDL58+Ufhnz0oMSrvc6twF5Gotau8jsfe2X92pP/khtIO3/eVy0e1uRnKtPx0FgSXcfqVwl1E+o2akw1sO/zRXa/e3VfDI299gMOdPcP2TPCaO6HX1ge4enwWS64c3aYtMT4mYpdvRmy4a7WMiHhhc+lxHnqjhNBjuK9tr+j0/f+2aCK56clt2ibnpZGTltzJJ/pGxIZ7K43cRcRrB2s+ZOXOtgG/p/IkT6w72Oln7p0/9py2SblpfTbdo3AXEblAB2pOUVff9uYo//1GCX/dVk5MuwupNQf/FLhuQjbQ9rWctCR++OlLwnpPXIW7iEgfeGdvNf/n5Z1nQ77V9iOBYwCDB8STmZLY5rXvXj/+gkf6ukG2iEgfuGxMJi/ed8U57ZV19fz05Z3UNzWf81pqcu/EsMJdRKSXZQ9K4ue3TuvT74z5+LeIiIjfKNxFRKKQwl1EJAp5Gu5mtsjMltbW1npZhohI1PE03J1zLzrnlqSlpXlZhohI1NG0jIhIFFK4i4hEIYW7iEgUiojLD5hZFXDgAj+eCVSHsRwvqS+RJ1r6AepLpOpJX0Y657I6eiEiwr0nzKyos2sr+I36EnmipR+gvkSq3uqLpmVERKKQwl1EJApFQ7gv9bqAMFJfIk+09APUl0jVK33x/Zy7iIicKxpG7iIi0o7CXUQkCvk63M1sgZntMrMSM7vf63o6Ymb7zex9M9tsZkXBtgwze83M9gR/Dw55//eC/dllZjeEtF8a3E6JmT1gZuG7EWPntT9qZpVmti2kLWy1m1mimT0VbF9nZgV93JcfmNmh4L7ZbGYLI70vZpZvZqvMbIeZFZvZPwbbfbdfztMXX+0XM0sys/VmtiXYjx8G273dJ845X/4AscBeYDSQAGwBJnpdVwd17gcy27X9B3B/8PH9wL8HH08M9iMRGBXsX2zwtfXAXAJ33f0rcGMf1D4PmAFs643agW8Avwk+vg14qo/78gPgnzp4b8T2BcgBZgQfDwJ2B+v13X45T198tV+C35kSfBwPrAPmeL1PejUcevMn+D/AqyHPvwd8z+u6OqhzP+eG+y4gJ/g4B9jVUR+AV4P9zAF2hrQvBn7bR/UX0DYQw1Z763uCj+MInKVnfdiXzkIk4vsSUsPzwCf8vF866Itv9wswAHgPmO31PvHztEwuUBryvCzYFmkcsMLMNprZkmDbUOfcEYDg7+xge2d9yg0+bt/uhXDWfvYzzrkmoBYY0muVd+xeM9sanLZp/bPZF30J/mk+ncBI0df7pV1fwGf7xcxizWwzUAm85pzzfJ/4Odw7mnOOxHWdlzvnZgA3Av9gZvPO897O+uSHvl5I7V736yFgDDANOAL8LNge8X0xsxTgf4BvOedOnO+tHbRFel98t1+cc83OuWlAHjDLzCad5+190g8/h3sZkB/yPA847FEtnXLOHQ7+rgSeBWYBFWaWAxD8XRl8e2d9Kgs+bt/uhXDWfvYzZhYHpAFHe63ydpxzFcF/lC3AwwT2TZu6giKqL2YWTyAMn3DOPRNs9uV+6agvft0vwdqPA28AC/B4n/g53DcA48xslJklEDjI8ILHNbVhZgPNbFDrY+B6YBuBOu8Kvu0uAnONBNtvCx4ZHwWMA9YH/6SrM7M5waPnXwz5TF8LZ+2h2/o8sNIFJxX7Qus/vKDPENg3rXVFZF+C3/sIsMM59/OQl3y3Xzrri9/2i5llmVl68HEycB2wE6/3SW8fJOnNH2AhgSPse4Hve11PB/WNJnBUfAtQ3Fojgbmy14E9wd8ZIZ/5frA/uwhZEQMUEviPfC/wa/rmANcyAn8WNxIYOdwdztqBJODPQAmBVQKj+7gvfwTeB7YG//HkRHpfgCsI/Dm+Fdgc/Fnox/1ynr74ar8AU4BNwXq3Af8abPd0n+jyAyIiUcjP0zIiItIJhbuISBRSuIuIRCGFu4hIFFK4i4hEIYW7iEgUUriLiESh/w/G9l4fFAbfEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df_freq['frequency'].plot()\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f283183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=20_000, replace=False, random_state=68)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eda52ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'../data/Data_clean_processed.csv', header=True, index=False)\n",
    "df_sample.to_csv(r'../data/Data_clean_processed_sample.csv', header=True, index=False)\n",
    "df_freq.to_csv('../data/clean_vocab.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01c36c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = {'df_clean': df, 'df_clean_sample': df_sample, 'clean_vocab': df_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9f884af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "with open('../data/data_clean_processed.pkl', 'wb') as handle:\n",
    "    pkl.dump(data_clean, handle, protocol=pkl.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf9463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a516d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342bb09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dc057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d12a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
